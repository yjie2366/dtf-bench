#!/bin/sh

SPLIT_WRAPPER=@split_wrapper@
top_dir=@abs_top_builddir@
bin_scale="${top_dir}/bin/scale"
bin_letkf="${top_dir}/bin/letkf"
scale_nodes=scale.nodes
letkf_nodes=letkf.nodes
bin_opts=()
log_dir=${top_dir}/log

trap_int()
{
	trap - SIGINT
	echo "Execution of the benchmark is interrupted!"
	exit
}

append_ppn()
{
	machine_file=$1
	ppn=$2

	sed -e "s/$/:${ppn}/" -i ${machine_file}
}

trap trap_int SIGINT

for opts; do
	if [ "${opts:0:1}" != "-" ]; then
		shift; continue
	else
		opts=`echo ${opts} | tr -d -`
		shift
	fi
	case $opts in
		imax)
			bin_opts+=("-imax=$1")
			;;
		jmax)
			bin_opts+=("-jmax=$1")
			;;
		kmax)
			bin_opts+=("-kmax=$1")
			;;
		lkmax)
			bin_opts+=("-lkmax=$1")
			;;
		ukmax)
			bin_opts+=("-ukmax=$1")
			;;
		okmax)
			bin_opts+=("-okmax=$1")
			;;
		cycles)
			bin_opts+=("-cycles=$1")
			;;
		px)
			bin_opts+=("-px=$1")
			;;
		py)
			bin_opts+=("-py=$1")
			;;
		member)
			member=$1
			bin_opts+=("-member=${member}")
			;;
		np)
			np=$1
			;;
		run)
			num_run=$1
			;;
		master)
			master=$1
			;;
		*)
			echo "$0 [ERROR]: Invalid option -$opts"
			exit
	esac
done

if [ -z "$np" ]; then np=4; fi
if [ -z "$member" ]; then member=2; fi
if [ -z "$master" ]; then master=2; fi
if [ -z "$num_run" ]; then num_run=1; fi

runlog_dir="${log_dir}/run-${np}-${master}-${member}"
output="${runlog_dir}/%n.%j.out"
error="${runlog_dir}/%n.%j.err"

# Number of processes per ensemble/member
SCALE_NP=$((np/member))
if [ ${master} -gt ${SCALE_NP} ];then
	master=${SCALE_NP}
fi
workgroup_size=$((SCALE_NP/master))

echo "Designated number of matcher: ${master}"
echo "Every ${workgroup_size} procs handled by a master process"
echo "Benchmark will run ${num_run} time(s)..."

ppn=${PJM_PROC_BY_NODE}
if [ -n "${ppn}" ]; then
	echo "Number of processes per node: ${ppn}"
else
	echo "[WARNING] Unable to retrieve ppn information"
fi

# Fugaku does not support this environment variable (Intel MPI only)
: << COMMENT
# Setup machine files (OFP)
find `pwd` \( -name "${scale_nodes}" -o -name "${letkf_nodes}" \) -exec rm {} \+

if [ -n "${I_MPI_HYDRA_HOST_FILE}" ]; then
	cat ${I_MPI_HYDRA_HOST_FILE} | head -n ${nnodes} > ${scale_nodes}
	cat ${I_MPI_HYDRA_HOST_FILE} | tail -n ${nnodes} > ${letkf_nodes}

	append_ppn ${scale_nodes} ${ppn}
	append_ppn ${letkf_nodes} ${ppn}

	mfargs_scale="-machinefile ${scale_nodes}"
	mfargs_letkf="-machinefile ${letkf_nodes}"
else
	echo "[Warning] machine file will be ignored"
fi
COMMENT

# Setup environment variables

export DTF_VERBOSE_LEVEL=0
export DTF_GLOBAL_PATH=${top_dir}
export DTF_IGNORE_IDLE=1
export MAX_WORKGROUP_SIZE=${workgroup_size}

# Setup Intel MPI version if necessary
if [[ "$(which mpiexec)" =~ .*"intel".* ]]; then
	# Verify Intel MPI version because the default ver.2019.5
	# may not work for DTF

	ppn_args="-ppn ${ppn}"
	genv_args="-genv LD_PRELOAD=${SPLIT_WRAPPER}/libsplitworld.so"

	impi_ver=2018.0.128 # works
	#impi_ver=2018.1.163 # works
	#impi_ver=2018.2.199 # works

	#impi_ver=2018.3.222 # NOT work NOT stable
	#impi_ver=2019.1.144 # NOT work
	#impi_ver=2019.3.199 # NOT work

	module switch intel/${impi_ver}
	echo "Intel MPI version: $(which mpiexec | grep -oE "[0-9]+.[0-9]+.[0-9]+")"
else
	export LD_PRELOAD=${SPLIT_WRAPPER}/libsplitworld.so
	export PLE_MPI_STD_EMPTYFILE="off"

	# Stop printf to the same log files
	#output_args="-stdout ${output} -stderr ${error}"
fi

# Start executing benchmark binaries
for run in `seq 1 ${num_run}`; do
	if [ -n "${SPLIT_WRAPPER}" ]; then
		mpiexec ${output_args} ${genv_args} ${ppn_args}\
		       	-n ${np} ${bin_scale} ${bin_opts[@]} :\
		       	-n ${np} ${bin_letkf} ${bin_opts[@]}
	else 
		mpiexec ${output_args} ${genv_args} ${ppn_args} \
			-n ${np} ${mfargs_scale} ${bin_scale} ${bin_opts[@]} &
		mpiexec	${output_args} ${genv_args} ${ppn_args} \
			-n ${np} ${mfargs_letkf} ${bin_letkf} ${bin_opts[@]}
	fi

	if [ "$?" -ne 0 ]; then
		echo "Error occurred. Please check error log!"
		break
	else
		echo "Execution succeeded!"
	fi
done

trap - SIGINT
